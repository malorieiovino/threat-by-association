# -*- coding: utf-8 -*-
"""data_collection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_szzm4zKV4qcBodwOLianEvqPY_P2wdW
"""

from google.colab import drive
drive.mount('/content/drive')

import os
project_path = '/content/drive/MyDrive/Research/AMLI'
os.makedirs(f'{project_path}/data/raw', exist_ok=True)
os.makedirs(f'{project_path}/data/processed', exist_ok=True)
os.makedirs(f'{project_path}/logs', exist_ok=True)

!pip install newspaper3k -q
!pip install nltk -q
!pip install spacy -q
!pip install lxml_html_clean -q
!pip install vaderSentiment -q
!pip install newsapi-python -q

import requests
import json
import pandas as pd
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
import numpy as np

import nltk
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

GUARDIAN_API_KEY = #commented out to push to Github
CNN_PATH = '/content/drive/MyDrive/Research/AMLI/data/cnn_corpus.csv'
SAVE_PATH = "/content/drive/MyDrive/Research/AMLI/data/guardian_corpus.csv"

"""### Search Parameters"""

SEARCH_TERMS_GUARDIAN = {
    "general": "(migrant OR migrants OR refugee OR refugees OR asylum OR immigrant OR immigrants OR displaced)",
    "ukraine": "(Ukraine OR Ukrainian)",
    "palestine": "(Palestine OR Palestinian OR Gaza OR Gazan)",
    "latinamerica": '(Mexico OR Mexican OR Venezuela OR Venezuelan OR "US border" OR caravan)'
}

SEARCH_TERMS_GUARDIAN = {
    'general': {
        'terms': [
            'refugee OR refugees',
            'migrant OR migrants',
            '"asylum seeker" OR "asylum seekers"',
            'displaced OR displacement',
            'immigrant OR immigrants OR immigration',
            'flee OR fleeing OR fled'
        ],
        'max_articles': 300
    },
    'ukrainian': {
        'terms': [
            'Ukraine AND (refugee OR refugees)',
            'Ukrainian AND (flee OR fled OR fleeing)',
            'Ukraine AND displaced'
        ],
        'max_articles': 200
    },
    'palestinian': {
        'terms': [
            'Gaza AND (refugee OR refugees)',
            'Palestinian AND displaced',
            'Gaza AND humanitarian',
            'Hamas'
        ],
        'max_articles': 200
    },
    'latinamerican': {
        'terms': [
            '"US border" AND (crossing OR migrant)',
            '"migrant caravan"',
            'Mexico AND (border OR migration)',
            'Venezuela AND (refugee OR migrant)'
        ],
        'max_articles': 200
    }
}

def collect_balanced_by_year(api_key):
    base_url = "https://content.guardianapis.com/search"
    all_articles = []

    years = ['2021', '2022', '2023', '2024', '2025']

    all_terms = []
    for category, config in SEARCH_TERMS_GUARDIAN.items():
        for term in config['terms']:
            all_terms.append((category, term))

    # Calculate articles per term per year to get ~1000 total
    total_target = 1000
    articles_per_year = total_target // len(years)  # 250 per year
    articles_per_term_per_year = articles_per_year // len(all_terms)  # ~10-15 per term per year

    print(f"Collecting {articles_per_term_per_year} articles per term per year")

    for year in years:
        print(f"\nYear {year}:")
        year_articles = 0

        for category, term in all_terms:
            params = {
                'api-key': api_key,
                'q': term,
                'from-date': f'{year}-01-01',
                'to-date': f'{year}-12-31',
                'page-size': articles_per_term_per_year,
                'show-fields': 'bodyText,headline,byline',
                'order-by': 'relevance'
            }

            response = requests.get(base_url, params=params)

            if response.status_code == 200:
                data = response.json()
                articles = data['response']['results'][:articles_per_term_per_year]

                for article in articles:
                    article['year'] = year
                    article['category'] = category
                    article['search_term'] = term

                all_articles.extend(articles)
                year_articles += len(articles)
                print(f"  {category} - {term[:30]}...: {len(articles)}")

            time.sleep(1)

        print(f"Year total: {year_articles}")

    return all_articles

GUARDIAN_API_KEY = #API Key commented out for Github push

guardian_balanced = collect_balanced_by_year(GUARDIAN_API_KEY)

df = pd.DataFrame(guardian_balanced)
print(f"\nTotal collected: {len(df)}")
print("\nBy year:")
print(df['year'].value_counts())
print("\nBy category:")
print(df['category'].value_counts())

df.to_csv('/content/drive/MyDrive/Research/AMLI/data/guardian_balanced_1000.csv', index=False)

cnn_path = "/content/drive/MyDrive/Research/AMLI/data/cnn_corpus.csv"
guardian_path = "/content/drive/MyDrive/Research/AMLI/data/guardian_balanced_1000.csv"

df_cnn = pd.read_csv(cnn_path)
df_guardian = pd.read_csv(guardian_path)

def profile(df, name):
    print(f"\n=== {name} ===")
    print("Shape:", df.shape)
    print("Columns:", df.columns.tolist())

    print("\nNull counts:")
    print(df.isna().sum())

    # duplicates
    if "url" in df:
        print("\nDuplicate URLs:", df['url'].duplicated().sum())
    if "title" in df:
        print("Duplicate titles:", df['title'].duplicated().sum())

    # date range
    if "date" in df:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        print("\nDate coverage:", df['date'].min(), "→", df['date'].max())
        print("By year:\n", df['date'].dt.year.value_counts().sort_index())

    # category/query distribution
    if "category" in df:
        print("\nCategory counts:")
        print(df['category'].value_counts())
    if "query" in df:
        print("\nQuery counts:")
        print(df['query'].value_counts())

    # text length stats
    if "text" in df:
        lens = df['text'].astype(str).str.split().str.len()
        print("\nText length (words):")
        print(lens.describe(percentiles=[.1,.25,.5,.75,.9]).round(1))
        print("Rows <100 words:", (lens < 100).sum())

# Run profiles
profile(df_cnn, "CNN Corpus")
profile(df_guardian, "Guardian Corpus")

QUERY_MAP = {
    "general": "(migrant OR migrants OR refugee OR refugees OR asylum OR immigrant OR immigrants OR displaced)",
    "ukraine": "(Ukraine OR Ukrainian)",
    "palestine": "(Palestine OR Palestinian OR Gaza OR Gazan)",
    "latinamerica": '(Mexico OR Mexican OR Venezuela OR Venezuelan OR "US border" OR caravan)',
}

def extract_body_from_fields(x):

    if isinstance(x, dict):
        return x.get('bodyText', '') or x.get('body', '')
    if isinstance(x, str):
        try:
            obj = ast.literal_eval(x)
            if isinstance(obj, dict):
                return obj.get('bodyText', '') or obj.get('body', '')
        except Exception:
            return ''
    return ''

def standardize_guardian(df, query_map=None):
    g = df.copy()
    if 'webPublicationDate' in g.columns: g['date']  = g['webPublicationDate']
    if 'webTitle'           in g.columns: g['title'] = g['webTitle']
    if 'webUrl'             in g.columns: g['url']   = g['webUrl']
    if 'text' not in g.columns:
        if 'fields' in g.columns:
            g['text'] = g['fields'].apply(extract_body_from_fields)
        else:
            g['text'] = ''
    g['outlet']   = 'Guardian'
    if 'language' not in g.columns:
        g['language'] = 'en'
    if 'query' not in g.columns:
        if 'search_term' in g.columns:
            g['query'] = g['search_term']
        elif query_map is not None and 'category' in g.columns:
            g['query'] = g['category'].map(query_map).fillna('')
        else:
            g['query'] = ''
    keep = ['outlet','category','query','date','title','url','language','text']
    for col in keep:
        if col not in g.columns:
            g[col] = ''
    return g[keep]

df_guardian_std = standardize_guardian(df_guardian, QUERY_MAP)

# sanity check
print("Guardian std shape:", df_guardian_std.shape)
print(df_guardian_std.isna().sum())
print(df_guardian_std.head(2)[['date','category','title','url','query']])

df_guardian_std['date'] = pd.to_datetime(df_guardian_std['date'], errors='coerce')

df_guardian_std = df_guardian_std.drop_duplicates(subset=['url']).reset_index(drop=True)
short_mask = df_guardian_std['text'].str.split().str.len() < 80
df_guardian_std = df_guardian_std[~short_mask].reset_index(drop=True)

print(df_guardian_std.shape)

cnn_path = "/content/drive/MyDrive/Research/AMLI/data/cnn_corpus_2021_2025.csv"
df_cnn = pd.read_csv(cnn_path)
df_cnn['outlet'] = 'CNN'
df_cnn['date'] = pd.to_datetime(df_cnn['date'], errors='coerce')

keep = ['outlet','category','query','date','title','url','language','text']
df_all = pd.concat([df_cnn[keep], df_guardian_std[keep]], ignore_index=True)

# sanity check
print(df_all.groupby(['outlet','category']).size())
print(df_all['date'].dt.year.value_counts().sort_index())

SAVE_ALL = "/content/drive/MyDrive/Research/AMLI/data/cnn_guardian_corpus.csv"
df_all.to_csv(SAVE_ALL, index=False)
print("Unified corpus saved:", SAVE_ALL)

all_path = "/content/drive/MyDrive/Research/AMLI/data/cnn_guardian_corpus.csv"
df_all = pd.read_csv(all_path)

def explore(df, name):
    print(f"\n=== {name} ===")
    print("Shape:", df.shape)
    print("Columns:", df.columns.tolist())
    print("\nNull counts:")
    print(df.isna().sum())

    # Duplicates
    if 'url' in df:
        print("\nDuplicate URLs:", df['url'].duplicated().sum())
    if 'title' in df:
        print("Duplicate titles:", df['title'].duplicated().sum())

    # Date coverage
    if 'date' in df:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        print("\nDate coverage:", df['date'].min(), "→", df['date'].max())
        print("By year:\n", df['date'].dt.year.value_counts().sort_index())

    # Outlet × Category counts
    if 'outlet' in df and 'category' in df:
        print("\nOutlet × Category counts:")
        print(df.groupby(['outlet','category']).size().unstack(fill_value=0))

    # Query distribution
    if 'query' in df:
        print("\nTop queries:")
        print(df['query'].value_counts().head(10))

    # Text length
    if 'text' in df:
        lens = df['text'].astype(str).str.split().str.len()
        print("\nText length stats:")
        print(lens.describe(percentiles=[.1,.25,.5,.75,.9]).round(1))
        print("Rows with <100 words:", (lens < 100).sum())

explore(df_all, "Unified CNN+Guardian Corpus")

cnn_path = "/content/drive/MyDrive/Research/AMLI/data/cnn_corpus_2021_2025.csv"
guardian_path = "/content/drive/MyDrive/Research/AMLI/data/guardian_balanced_1000.csv"

df_cnn = pd.read_csv(cnn_path)
df_guardian = pd.read_csv(guardian_path)

def quick_summary(df, name):
    out = {}
    out['rows'] = df.shape[0]
    out['cols'] = df.shape[1]
    if 'date' in df:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        out['date_min'] = df['date'].min()
        out['date_max'] = df['date'].max()
        out['years'] = df['date'].dt.year.value_counts().sort_index().to_dict()
    if 'category' in df:
        out['categories'] = df['category'].value_counts().to_dict()
    if 'url' in df:
        out['dup_urls'] = df['url'].duplicated().sum()
    if 'title' in df:
        out['dup_titles'] = df['title'].duplicated().sum()
    if 'text' in df:
        lens = df['text'].astype(str).str.split().str.len()
        out['text_mean'] = lens.mean()
        out['text_min'] = lens.min()
        out['text_max'] = lens.max()
    return pd.Series(out, name=name)

summary = pd.concat(
    [quick_summary(df_cnn, "CNN"), quick_summary(df_guardian_std, "Guardian")],
    axis=1
)
print(summary)

df_cnn['date'] = pd.to_datetime(df_cnn['date'], errors='coerce')
df_guardian_std['date'] = pd.to_datetime(df_guardian_std['date'], errors='coerce')
fig, ax = plt.subplots(1, 2, figsize=(12,5), sharey=True)

df_cnn['category'].value_counts().plot(kind='bar', ax=ax[0], color='skyblue', title="CNN Category Counts")
df_guardian_std['category'].value_counts().plot(kind='bar', ax=ax[1], color='lightcoral', title="Guardian Category Counts")

plt.tight_layout()
plt.show()

fig, ax = plt.subplots(1, 2, figsize=(12,5), sharey=True)

df_cnn['date'].dt.year.value_counts().sort_index().plot(kind='bar', ax=ax[0], color='skyblue', title="CNN Year Counts")
df_guardian_std['date'].dt.year.value_counts().sort_index().plot(kind='bar', ax=ax[1], color='lightcoral', title="Guardian Year Counts")

plt.tight_layout()
plt.show()