# -*- coding: utf-8 -*-
"""evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BgaLCWbbg0xe5Bsc1u6VtUZtBd1obJXA
"""

from google.colab import drive
drive.mount('/content/drive')


import pandas as pd
import numpy as np
from collections import Counter, defaultdict
import re
from typing import Dict, List, Tuple
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder, TrigramAssocMeasures, TrigramCollocationFinder
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
nltk.download('punkt_tab', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)

DATA_PATH = "/content/drive/MyDrive/Research/AMLI/data/cnn_guardian_corpus.csv"

df = pd.read_csv(DATA_PATH)
df['date'] = pd.to_datetime(df['date'], errors='coerce')
df['text'] = df['text'].astype(str)
df['text_lower'] = df['text'].str.lower()

print("Shape:", df.shape)
print(df.head(3))

#sanity check
print(df['outlet'].value_counts())
print(df['category'].value_counts())
print(df['date'].min(), "â†’", df['date'].max())

"""----

# LEXICAL ANALYSIS
"""

class FocusedLexicalAnalyzer:
    """Focused analysis on 4 key groups"""

    def __init__(self, df):
        self.df = df.copy()

        # 4 key groups: palestinians, ukrainians, mexicans, venezuelans
        self.groups = {
            'palestinian': ['palestinian', 'palestinians', 'gaza', 'gazan', 'gazans', 'west bank'],
            'ukrainian': ['ukrainian', 'ukrainians', 'ukraine'],
            'mexican': ['mexican', 'mexicans', 'mexico'],
            'venezuelan': ['venezuelan', 'venezuelans', 'venezuela']
        }

        self.status_terms = {
            'refugee': ['refugee', 'refugees', 'asylum seeker', 'asylum seekers', 'asylum-seeker'],
            'migrant': ['migrant', 'migrants', 'immigrant', 'immigrants', 'immigration'],
            'illegal': ['illegal', 'illegals', 'undocumented', 'unauthorized', 'irregular', 'alien', 'aliens'],
            'displaced': ['displaced', 'evacuee', 'evacuated', 'fled', 'fleeing', 'escapee']
        }

        self.action_verbs = {
            'agency_positive': ['flee', 'fled', 'fleeing', 'escape', 'escaping', 'escaped',
                              'seek', 'seeking', 'sought', 'rescue', 'rescued', 'save', 'saved'],
            'agency_negative': ['flood', 'flooding', 'flooded', 'pour', 'pouring', 'poured',
                              'surge', 'surging', 'surged', 'swarm', 'swarming', 'swarmed',
                              'invade', 'invading', 'invaded', 'infiltrate', 'infiltrating'],
            'passive_victim': ['killed', 'died', 'drowned', 'perished', 'lost', 'trapped',
                             'stranded', 'abandoned', 'attacked', 'bombed', 'targeted',
                             'wounded', 'injured', 'harmed', 'destroyed'],
            'state_control': ['detained', 'deported', 'processed', 'apprehended', 'intercepted',
                            'blocked', 'stopped', 'expelled', 'removed', 'arrested', 'held',
                            'caught', 'captured', 'rounded up']
        }

        self.threat_metaphors = {
            'water': ['flood', 'flooding', 'flooded', 'wave', 'waves', 'surge', 'surging',
                     'flow', 'flowing', 'stream', 'tide', 'influx', 'pour', 'pouring',
                     'tsunami', 'deluge', 'inundation', 'overflow'],
            'invasion': ['invasion', 'invading', 'invaders', 'swarm', 'swarming', 'horde',
                        'mass', 'masses', 'army', 'siege', 'infiltrate', 'infiltration',
                        'breach', 'breached'],
            'burden': ['burden', 'strain', 'overwhelming', 'crisis', 'pressure', 'challenge',
                      'threat', 'problem', 'chaos', 'emergency', 'catastrophe', 'disaster']
        }

        self.evaluative_adjectives = {
            'positive': ['desperate', 'vulnerable', 'innocent', 'brave', 'legitimate',
                        'genuine', 'peaceful', 'heroic', 'courageous', 'worthy'],
            'negative': ['illegal', 'bogus', 'fake', 'criminal', 'dangerous', 'threatening',
                        'hostile', 'aggressive', 'violent', 'suspicious', 'fraudulent'],
            'neutral': ['young', 'elderly', 'male', 'female', 'foreign', 'local']
        }

    def get_group_articles(self, group):
        """Extract all articles mentioning a specific group"""
        terms = self.groups[group]
        pattern = '|'.join([r'\b' + term + r'\b' for term in terms])
        mask = self.df['text'].str.contains(pattern, case=False, na=False, regex=True)
        return self.df[mask]

    def analyze_lexical_choices(self):
        """Main lexical analysis"""
        results = []

        for group in self.groups.keys():
            group_docs = self.get_group_articles(group)

            if len(group_docs) == 0:
                continue

            status_counts = {}
            for status, terms in self.status_terms.items():
                count = 0
                for term in terms:
                    count += group_docs['text'].str.count(
                        r'\b' + re.escape(term) + r's?\b',
                        flags=re.IGNORECASE
                    ).sum()
                status_counts[status] = count

           #action verbs
            action_counts = {}
            for action_type, verbs in self.action_verbs.items():
                count = 0
                for verb in verbs:
                    count += group_docs['text'].str.count(
                        r'\b' + re.escape(verb) + r'\b',
                        flags=re.IGNORECASE
                    ).sum()
                action_counts[action_type] = count

            # metaphors
            threat_counts = {}
            for threat_type, terms in self.threat_metaphors.items():
                count = 0
                for term in terms:
                    count += group_docs['text'].str.count(
                        r'\b' + re.escape(term) + r'\b',
                        flags=re.IGNORECASE
                    ).sum()
                threat_counts[threat_type] = count

            # evaluative adjectives
            eval_counts = {}
            for eval_type, adjs in self.evaluative_adjectives.items():
                count = 0
                for adj in adjs:
                    count += group_docs['text'].str.count(
                        r'\b' + re.escape(adj) + r'\b',
                        flags=re.IGNORECASE
                    ).sum()
                eval_counts[eval_type] = count

            result = {
                'group': group,
                'n_articles': len(group_docs),
                'cnn_articles': len(group_docs[group_docs['outlet'] == 'CNN']),
                'guardian_articles': len(group_docs[group_docs['outlet'] == 'Guardian']),
                **{f'status_{k}': v for k, v in status_counts.items()},
                **{f'action_{k}': v for k, v in action_counts.items()},
                **{f'threat_{k}': v for k, v in threat_counts.items()},
                **{f'eval_{k}': v for k, v in eval_counts.items()}
            }

            results.append(result)

        return pd.DataFrame(results)

    def calculate_percentages(self, df):
        """Calculate percentage distributions"""
        df = df.copy()

        # Status term percentages
        status_cols = ['status_refugee', 'status_migrant', 'status_illegal', 'status_displaced']
        df['status_total'] = df[status_cols].sum(axis=1)
        for col in status_cols:
            new_col = col.replace('status_', '') + '_pct'
            df[new_col] = (df[col] / df['status_total'] * 100).round(1)

        # Action verb percentages
        action_cols = ['action_agency_positive', 'action_agency_negative',
                      'action_passive_victim', 'action_state_control']
        df['action_total'] = df[action_cols].sum(axis=1)
        for col in action_cols:
            new_col = col.replace('action_', '') + '_pct'
            df[new_col] = (df[col] / df['action_total'] * 100).round(1)

        # Threat density (per article)
        df['threat_total'] = df[['threat_water', 'threat_invasion', 'threat_burden']].sum(axis=1)
        df['threat_per_article'] = (df['threat_total'] / df['n_articles']).round(2)

        # Evaluation balance
        df['eval_total'] = df[['eval_positive', 'eval_negative', 'eval_neutral']].sum(axis=1)
        df['eval_positive_pct'] = (df['eval_positive'] / df['eval_total'] * 100).round(1)
        df['eval_negative_pct'] = (df['eval_negative'] / df['eval_total'] * 100).round(1)

        # Key indices
        df['humanization_index'] = ((df['status_refugee'] + df['status_displaced']) /
                                    df['status_total'] * 100).round(1)
        df['criminalization_index'] = ((df['status_illegal'] + df['action_state_control']) /
                                       (df['status_total'] + df['action_total']) * 100).round(1)
        df['victim_index'] = (df['action_passive_victim'] / df['action_total'] * 100).round(1)
        df['agency_index'] = (df['action_agency_positive'] / df['action_total'] * 100).round(1)

        return df

    def compare_outlets(self):
        """Compare CNN vs Guardian for each group"""
        comparison = []

        for group in self.groups.keys():
            for outlet in ['CNN', 'Guardian']:
                outlet_df = self.df[self.df['outlet'] == outlet]
                group_docs = outlet_df[outlet_df['text'].str.contains(
                    '|'.join([r'\b' + t + r'\b' for t in self.groups[group]]),
                    case=False, na=False, regex=True)]

                if len(group_docs) == 0:
                    continue

                # normfreq calc
                total_words = group_docs['text'].str.split().str.len().sum()

                result = {'group': group, 'outlet': outlet, 'n_articles': len(group_docs)}

                # Key term frequencies per 1000 words
                for label, term in [
                    ('refugee', 'refugee'),
                    ('migrant', 'migrant'),
                    ('illegal', 'illegal'),
                    ('crisis', 'crisis'),
                    ('flood', 'flood'),
                    ('victim', 'victim'),
                    ('civilian', 'civilian'),
                    ('terrorist', 'terrorist')
                ]:
                    count = group_docs['text'].str.count(
                        r'\b' + term + r's?\b',
                        flags=re.IGNORECASE
                    ).sum()
                    result[f'{label}_per_1000'] = (count / total_words * 1000) if total_words > 0 else 0

                comparison.append(result)

        return pd.DataFrame(comparison)

analyzer = FocusedLexicalAnalyzer(df)

print("\n1. ANALYZING LEXICAL CHOICES...")
lexical_df = analyzer.analyze_lexical_choices()
lexical_df = analyzer.calculate_percentages(lexical_df)
group_order = ['palestinian', 'ukrainian', 'mexican', 'venezuelan']
lexical_df['group_order'] = lexical_df['group'].map({g: i for i, g in enumerate(group_order)})
lexical_df = lexical_df.sort_values('group_order').drop('group_order', axis=1)

print("\n2. COMPARING OUTLETS...")
outlet_df = analyzer.compare_outlets()
print("\n" + "="*70)
print("RESULTS: STATUS TERM DISTRIBUTION")
print("="*70)
print(lexical_df[['group', 'n_articles', 'refugee_pct', 'migrant_pct',
                  'illegal_pct', 'displaced_pct']].to_string(index=False))

print("\n" + "="*70)
print("RESULTS: ACTION PATTERNS")
print("="*70)
print(lexical_df[['group', 'agency_positive_pct', 'agency_negative_pct',
                  'passive_victim_pct', 'state_control_pct']].to_string(index=False))

print("\n" + "="*70)
print("RESULTS: KEY INDICES")
print("="*70)
print(lexical_df[['group', 'humanization_index', 'criminalization_index',
                  'victim_index', 'agency_index', 'threat_per_article']].to_string(index=False))

print("\n" + "="*70)
print("RESULTS: CNN vs GUARDIAN")
print("="*70)
for group in group_order:
    group_data = outlet_df[outlet_df['group'] == group]
    if len(group_data) > 0:
        print(f"\n{group.upper()}:")
        print(group_data[['outlet', 'refugee_per_1000', 'migrant_per_1000',
                         'illegal_per_1000', 'crisis_per_1000']].to_string(index=False))

# Statistical comparison
print("\n" + "="*70)
print("STATISTICAL COMPARISON")
print("="*70)

from scipy.stats import chi2_contingency

# Palestinians vs Ukrainians
pal = lexical_df[lexical_df['group'] == 'palestinian'].iloc[0]
ukr = lexical_df[lexical_df['group'] == 'ukrainian'].iloc[0]

contingency = [
    [pal['status_refugee'], pal['status_migrant'], pal['status_illegal']],
    [ukr['status_refugee'], ukr['status_migrant'], ukr['status_illegal']]
]
chi2, p, dof, expected = chi2_contingency(contingency)
print(f"\nPalestinian vs Ukrainian status terms: Ï‡Â² = {chi2:.2f}, p = {p:.4f}")

# Mexicans vs Venezuelans
mex = lexical_df[lexical_df['group'] == 'mexican'].iloc[0]
ven = lexical_df[lexical_df['group'] == 'venezuelan'].iloc[0]

contingency = [
    [mex['status_refugee'], mex['status_migrant'], mex['status_illegal']],
    [ven['status_refugee'], ven['status_migrant'], ven['status_illegal']]
]
chi2, p, dof, expected = chi2_contingency(contingency)
print(f"Mexican vs Venezuelan status terms: Ï‡Â² = {chi2:.2f}, p = {p:.4f}")

lexical_results = lexical_df
outlet_results = outlet_df

print("\nâœ“ Lexical analysis complete. Run collocation analysis next.")
print("âœ“ Results stored in: lexical_results, outlet_results")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Rectangle
import matplotlib.patches as mpatches

plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.dpi'] = 100
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 11
plt.rcParams['font.family'] = 'sans-serif'

fig = plt.figure(figsize=(16, 12))

# color scheme
colors = {
    'palestinian': '#E74C3C',  # Red
    'ukrainian': '#3498DB',    # Blue
    'mexican': '#2ECC71',      # Green
    'venezuelan': '#F39C12'    # Orange
}

# order display
group_order = ['palestinian', 'ukrainian', 'mexican', 'venezuelan']
group_labels = ['Palestinian', 'Ukrainian', 'Mexican', 'Venezuelan']

# status term distribution
ax1 = plt.subplot(2, 3, 1)

status_data = lexical_results[['group', 'refugee_pct', 'migrant_pct', 'illegal_pct', 'displaced_pct']]
status_data = status_data.set_index('group').reindex(group_order)

status_data.plot(kind='bar', stacked=True, ax=ax1,
                 color=['#2E7D32', '#1976D2', '#D32F2F', '#F57C00'],
                 width=0.7)

ax1.set_title('Status Term Distribution', fontsize=14, fontweight='bold', pad=10)
ax1.set_xlabel('')
ax1.set_ylabel('Percentage of Status Terms (%)', fontsize=11)
ax1.set_xticklabels(group_labels, rotation=0, ha='center')
ax1.legend(title='Category', labels=['Refugee', 'Migrant', 'Illegal', 'Displaced'],
          loc='upper right', fontsize=9, title_fontsize=10)
ax1.set_ylim(0, 100)
ax1.grid(axis='y', alpha=0.3, linestyle='--')


for container in ax1.containers:
    ax1.bar_label(container, fmt='%.0f', label_type='center', fontsize=9, weight='bold')

# agency vs victimization
ax2 = plt.subplot(2, 3, 2)

agency_cols = ['agency_index', 'victim_index']
agency_data = lexical_results[['group'] + agency_cols]
agency_data = agency_data.set_index('group').reindex(group_order)

x = np.arange(len(group_order))
width = 0.35

bars1 = ax2.bar(x - width/2, agency_data['agency_index'], width,
                label='Positive Agency', color='#4CAF50', alpha=0.8)
bars2 = ax2.bar(x + width/2, agency_data['victim_index'], width,
                label='Passive Victim', color='#FF5722', alpha=0.8)

ax2.set_title('Agency vs Victimization Patterns', fontsize=14, fontweight='bold', pad=10)
ax2.set_xlabel('')
ax2.set_ylabel('Percentage of Action Verbs (%)', fontsize=11)
ax2.set_xticks(x)
ax2.set_xticklabels(group_labels, rotation=0)
ax2.legend(loc='upper right', fontsize=10)
ax2.grid(axis='y', alpha=0.3, linestyle='--')
ax2.bar_label(bars1, fmt='%.1f%%', padding=3, fontsize=9)
ax2.bar_label(bars2, fmt='%.1f%%', padding=3, fontsize=9)

# humanization vs criminalization
ax3 = plt.subplot(2, 3, 3)

for i, group in enumerate(group_order):
    data = lexical_results[lexical_results['group'] == group].iloc[0]
    ax3.scatter(data['criminalization_index'], data['humanization_index'],
               s=data['n_articles']*2, color=colors[group], alpha=0.7,
               label=f"{group_labels[i]} (n={int(data['n_articles'])})")


    ax3.annotate(group_labels[i],
                (data['criminalization_index'], data['humanization_index']),
                xytext=(5, 5), textcoords='offset points', fontsize=10)

ax3.set_title('Humanization vs Criminalization', fontsize=14, fontweight='bold', pad=10)
ax3.set_xlabel('Criminalization Index (%)', fontsize=11)
ax3.set_ylabel('Humanization Index (%)', fontsize=11)
ax3.legend(loc='best', fontsize=9, title='Group (articles)', title_fontsize=10)
ax3.grid(True, alpha=0.3, linestyle='--')
ax3.axvline(x=14, color='gray', linestyle=':', alpha=0.5)
ax3.axhline(y=45, color='gray', linestyle=':', alpha=0.5)

# threat language density
ax4 = plt.subplot(2, 3, 4)
threat_data = lexical_results[['group', 'threat_per_article']].set_index('group').reindex(group_order)
bars = ax4.bar(range(len(group_order)), threat_data['threat_per_article'],
               color=[colors[g] for g in group_order], alpha=0.8, edgecolor='black', linewidth=1.5)

ax4.set_title('Threat Metaphor Density', fontsize=14, fontweight='bold', pad=10)
ax4.set_xlabel('')
ax4.set_ylabel('Threat Terms per Article', fontsize=11)
ax4.set_xticks(range(len(group_order)))
ax4.set_xticklabels(group_labels, rotation=0)
ax4.grid(axis='y', alpha=0.3, linestyle='--')
for bar in bars:
    height = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.2f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
mean_threat = threat_data['threat_per_article'].mean()
ax4.axhline(y=mean_threat, color='red', linestyle='--', alpha=0.5, label=f'Mean: {mean_threat:.2f}')
ax4.legend(loc='upper right', fontsize=10)

# CNN vs Guardian
ax5 = plt.subplot(2, 3, 5)
outlet_terms = ['refugee_per_1000', 'migrant_per_1000', 'illegal_per_1000']
outlet_labels = ['Refugee', 'Migrant', 'Illegal']

differences = {}
for group in ['palestinian', 'ukrainian']:
    cnn_data = outlet_results[(outlet_results['group'] == group) & (outlet_results['outlet'] == 'CNN')]
    guardian_data = outlet_results[(outlet_results['group'] == group) & (outlet_results['outlet'] == 'Guardian')]

    if len(cnn_data) > 0 and len(guardian_data) > 0:
        diff = []
        for term in outlet_terms:
            diff.append(guardian_data.iloc[0][term] - cnn_data.iloc[0][term])
        differences[group] = diff

x = np.arange(len(outlet_labels))
width = 0.35

if 'palestinian' in differences and 'ukrainian' in differences:
    bars1 = ax5.bar(x - width/2, differences['palestinian'], width,
                    label='Palestinian', color=colors['palestinian'], alpha=0.8)
    bars2 = ax5.bar(x + width/2, differences['ukrainian'], width,
                    label='Ukrainian', color=colors['ukrainian'], alpha=0.8)

    ax5.set_title('Outlet Bias: Guardian vs CNN\n(positive = Guardian uses term more)',
                 fontsize=14, fontweight='bold', pad=10)
    ax5.set_xlabel('Term Category', fontsize=11)
    ax5.set_ylabel('Difference per 1000 words', fontsize=11)
    ax5.set_xticks(x)
    ax5.set_xticklabels(outlet_labels)
    ax5.legend(loc='best', fontsize=10)
    ax5.grid(axis='y', alpha=0.3, linestyle='--')
    ax5.axhline(y=0, color='black', linestyle='-', linewidth=1)
    ax5.bar_label(bars1, fmt='%.2f', padding=3, fontsize=9)
    ax5.bar_label(bars2, fmt='%.2f', padding=3, fontsize=9)

# Key indices
ax6 = plt.subplot(2, 3, 6)

indices = ['humanization_index', 'criminalization_index', 'victim_index', 'agency_index']
index_labels = ['Humanization', 'Criminalization', 'Victimization', 'Agency']

heatmap_data = lexical_results[['group'] + indices].set_index('group').reindex(group_order)
heatmap_data.columns = index_labels

# heatmap
im = ax6.imshow(heatmap_data.T, cmap='RdYlGn_r', aspect='auto', vmin=0, vmax=80)

ax6.set_xticks(range(len(group_order)))
ax6.set_yticks(range(len(index_labels)))
ax6.set_xticklabels(group_labels, rotation=0)
ax6.set_yticklabels(index_labels)

for i in range(len(index_labels)):
    for j in range(len(group_order)):
        text = ax6.text(j, i, f'{heatmap_data.iloc[j, i]:.1f}%',
                       ha="center", va="center", color="white", fontsize=10, fontweight='bold')

ax6.set_title('Comparative Index Heatmap', fontsize=14, fontweight='bold', pad=10)

cbar = plt.colorbar(im, ax=ax6, fraction=0.046, pad=0.04)
cbar.set_label('Percentage (%)', rotation=270, labelpad=15)

# title
fig.suptitle('Lexical Analysis: Differential Framing of Displaced Populations',
            fontsize=16, fontweight='bold', y=1.02)

plt.tight_layout()

output_path = "/content/drive/MyDrive/Research/AMLI/figures/"
import os
os.makedirs(output_path, exist_ok=True)
plt.savefig(f'{output_path}lexical_analysis_comprehensive.png', dpi=300, bbox_inches='tight')
plt.savefig(f'{output_path}lexical_analysis_comprehensive.pdf', bbox_inches='tight')
plt.show()

# summary table
print("\n" + "="*70)
print("SUMMARY TABLE FOR PAPER")
print("="*70)

# latex ready table
summary_table = lexical_results[['group', 'n_articles', 'humanization_index',
                                 'criminalization_index', 'victim_index', 'agency_index']].copy()
summary_table['group'] = summary_table['group'].str.capitalize()
summary_table.columns = ['Group', 'N', 'Humanization (%)', 'Criminalization (%)',
                         'Victimization (%)', 'Agency (%)']

print("\nMarkdown format:")
print(summary_table.to_markdown(index=False, floatfmt='.1f'))

print("\nLaTeX format:")
print(summary_table.to_latex(index=False, float_format='%.1f', escape=False))

print("\nâœ“ Visualizations saved to:", output_path)
print("âœ“ Formats: PNG (for viewing) and PDF (for publication)")

"""-----

# COLLOCATION ANALYSIS
"""

# low salience words
low_salience_words = {
    # Reporting verbs
    'said', 'says', 'saying', 'told', 'tells', 'telling', 'according',
    'reported', 'reports', 'reporting', 'stated', 'states', 'stating',
    'announced', 'announces', 'announcing', 'claimed', 'claims', 'claiming',
    'added', 'adds', 'adding', 'noted', 'notes', 'noting', 'asked', 'asking',

    # Common function words
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
    'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
    'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these',
    'those', 'there', 'their', 'them', 'they', 'we', 'our', 'us', 'it',
    'its', 'he', 'she', 'her', 'his', 'him', 'who', 'what', 'when',
    'where', 'which', 'why', 'how', 'all', 'some', 'any', 'both', 'each',
    'more', 'most', 'other', 'such', 'only', 'own', 'same', 'so', 'than',
    'too', 'very', 'just', 'now', 'also', 'about', 'out', 'up', 'down',
    'into', 'after', 'before', 'between', 'through', 'during', 'while',
    'not', 'no', 'nor', 'neither', 'either', 'yes',

    # Generic/low-meaning words
    'people', 'person', 'year', 'years', 'time', 'day', 'days', 'week',
    'month', 'months', 'group', 'groups', 'number', 'numbers', 'part',
    'area', 'areas', 'place', 'places', 'thing', 'things', 'way', 'ways',
    'many', 'much', 'several', 'few', 'including', 'since', 'still',
    'even', 'back', 'well', 'over', 'under', 'around', 'near', 'far',
    'new', 'old', 'good', 'bad', 'great', 'little', 'big', 'small',
    'along', 'across', 'against', 'among', 'within', 'without',
    'nearly', 'almost', 'quite', 'rather', 'really', 'already',
    'yet', 'ever', 'never', 'always', 'often', 'sometimes',
    'city', 'cities', 'town', 'towns', 'village', 'villages',
    'today', 'yesterday', 'tomorrow', 'tonight', 'morning', 'night',

    # Numbers and quantities
    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten',
    'first', 'second', 'third', 'last', 'next', 'previous',
    'hundred', 'thousand', 'million', 'billion',
    '000', '250', '350', 'cnn', 'guardian', 'reuters', 'news',

    # More generic verbs
    'come', 'comes', 'coming', 'came', 'go', 'goes', 'going', 'went',
    'make', 'makes', 'making', 'made', 'take', 'takes', 'taking', 'took',
    'get', 'gets', 'getting', 'got', 'give', 'gives', 'giving', 'gave',
    'say', 'see', 'saw', 'seen', 'know', 'knows', 'known', 'think', 'thought',
    'want', 'wanted', 'need', 'needed', 'use', 'used', 'find', 'found',
    'tell', 'told', 'become', 'became', 'leave', 'left', 'feel', 'felt',
    'bring', 'brought', 'begin', 'began', 'keep', 'kept', 'let', 'put',
    'seem', 'seemed', 'run', 'ran', 'move', 'moved', 'live', 'lived',
    'believe', 'believed', 'happen', 'happened', 'include', 'included', 'travel',
    'mostly', 'southern', 'united', 'being'
}

# Words to specifically keep (override filter)
meaningful_words = {
    # Violence/conflict
    'killed', 'bombing', 'attack', 'war', 'military', 'forces', 'invasion',
    'defense', 'weapons', 'violence', 'death', 'strike', 'airstrike',

    # Movement/displacement
    'border', 'crossing', 'refugee', 'refugees', 'migrants', 'immigration',
    'asylum', 'flee', 'fled', 'escape', 'deport', 'deported', 'detention',

    # Political
    'president', 'government', 'opposition', 'authorities', 'official',
    'officials', 'administration', 'minister', 'leader', 'regime',

    # Geographic (specific)
    'gaza', 'israel', 'israeli', 'hamas', 'ukraine', 'ukrainian', 'russia',
    'russian', 'mexico', 'mexican', 'venezuela', 'venezuelan', 'border',

    # Humanitarian
    'humanitarian', 'aid', 'crisis', 'emergency', 'hospital', 'medical',
    'children', 'families', 'shelter', 'camp', 'camps'
}
groups = {
    'palestinian': ['palestinian', 'palestinians', 'gaza', 'gazan', 'gazans'],
    'ukrainian': ['ukrainian', 'ukrainians', 'ukraine'],
    'mexican': ['mexican', 'mexicans', 'mexico'],
    'venezuelan': ['venezuelan', 'venezuelans', 'venezuela']
}

for group_name, group_terms in groups.items():
    print(f"\n{group_name.upper()}:")

    pattern = '|'.join([r'\b' + term + r'\b' for term in group_terms])
    group_docs = df[df['text'].str.contains(pattern, case=False, na=False, regex=True)]

    if len(group_docs) == 0:
        print("  No articles found")
        continue

    print(f"  Articles analyzed: {len(group_docs)}")

    left_collocations = Counter()
    right_collocations = Counter()

    # collocation extraction
    for text in group_docs['text'].dropna():
        text_lower = text.lower()
        words = text_lower.split()

        for term in group_terms:
            for i, word in enumerate(words):
                if term in word:
                    # Words BEFORE
                    for j in range(max(0, i-5), i):
                        w = words[j].strip('.,!?;:"()[]{}')
                        if w not in low_salience_words and len(w) > 3:
                            if group_name == 'mexican' and w == 'new':
                                continue
                            if not w.isdigit():
                                left_collocations[w] += 1

                    # Words AFTER
                    for j in range(i+1, min(len(words), i+6)):
                        w = words[j].strip('.,!?;:"()[]{}')
                        if w not in low_salience_words and len(w) > 3:
                            if group_name == 'mexican' and w == 'new':
                                continue
                            if not w.isdigit():
                                right_collocations[w] += 1

    print(f"\n  Words BEFORE {group_name}:")
    for word, count in left_collocations.most_common(10):
        print(f"    {word}: {count}")

    print(f"\n  Words AFTER {group_name}:")
    for word, count in right_collocations.most_common(10):
        print(f"    {word}: {count}")

print("\n" + "="*70)
print("Analysis complete - clean, meaningful collocations extracted")
print("="*70)

plt.style.use('default')
sns.set_palette("husl")

collocation_data = {
    'palestinian': {
        'before': [('israeli', 362), ('northern', 272), ('killed', 267), ('gaza', 245),
                   ('israel', 224), ('humanitarian', 197), ('hamas', 195), ('least', 182),
                   ('israel\'s', 181), ('palestinians', 162)],
        'after': [('israeli', 417), ('gaza', 400), ('strip', 368), ('killed', 346),
                  ('israel', 301), ('health', 259), ('ministry', 196), ('authority', 193),
                  ('hamas', 181), ('state', 137)]
    },
    'ukrainian': {
        'before': [('russia', 249), ('russian', 173), ('support', 144), ('invasion', 139),
                   ('ukraine', 124), ('military', 106), ('russia\'s', 89), ('help', 74),
                   ('border', 72), ('kyiv', 69)],
        'after': [('president', 222), ('russian', 196), ('russia', 190), ('forces', 163),
                  ('volodymyr', 157), ('zelensky', 131), ('military', 128), ('defense', 109),
                  ('official', 108), ('ukrainian', 100)]
    },
    'mexican': {
        'before': [('border', 90), ('migrants', 75), ('tariffs', 35), ('mexico', 33),
                   ('canada', 32), ('crossing', 22), ('center', 21), ('islamic', 18),
                   ('asylum', 18), ('migration', 17)],
        'after': [('border', 270), ('president', 67), ('government', 46), ('officials', 44),
                  ('canada', 42), ('authorities', 39), ('migrants', 35), ('sheinbaum', 31),
                  ('mexico', 27), ('claudia', 27)]
    },
    'venezuelan': {
        'before': [('temporary', 17), ('350,000', 17), ('border', 16), ('migrants', 15),
                   ('haitians', 15), ('held', 14), ('flights', 14), ('alleged', 14),
                   ('granted', 13), ('homes', 13)],
        'after': [('migrants', 67), ('government', 33), ('president', 31), ('opposition', 26),
                  ('deported', 24), ('abroad', 23), ('gang', 21), ('refugees', 20),
                  ('border', 20), ('leader', 18)]
    }
}

fig = plt.figure(figsize=(18, 12))

# top words table
ax1 = plt.subplot(2, 3, (1, 2))
ax1.axis('tight')
ax1.axis('off')

table_data = []
for group in ['palestinian', 'ukrainian', 'mexican', 'venezuelan']:
    # Top 5 before and after
    before_words = ', '.join([f"{word}({count})" for word, count in collocation_data[group]['before'][:5]])
    after_words = ', '.join([f"{word}({count})" for word, count in collocation_data[group]['after'][:5]])
    table_data.append([group.capitalize(), before_words, after_words])

table = ax1.table(cellText=table_data,
                  colLabels=['Group', 'Top Words BEFORE', 'Top Words AFTER'],
                  cellLoc='left',
                  loc='center',
                  colWidths=[0.12, 0.44, 0.44])

table.auto_set_font_size(False)
table.set_fontsize(9)
table.scale(1, 2)

for i in range(3):
    table[(0, i)].set_facecolor('#40466e')
    table[(0, i)].set_text_props(weight='bold', color='white')

colors = ['#FFE5E5', '#E5F0FF', '#E5FFE5', '#FFF5E5']
for i in range(1, 5):
    for j in range(3):
        table[(i, j)].set_facecolor(colors[i-1])

ax1.set_title('Top Collocations Summary Table', fontsize=14, fontweight='bold', pad=20)

# Before/After
ax2 = plt.subplot(2, 3, 3)

# Calculate ratio of before vs after for key terms
groups = ['Palestinian', 'Ukrainian', 'Mexican', 'Venezuelan']
colors_bars = ['#E74C3C', '#3498DB', '#2ECC71', '#F39C12']


patterns = {
    'Palestinian': {'killed': (267, 346), 'israeli': (362, 417), 'gaza': (245, 400)},
    'Ukrainian': {'president': (0, 222), 'military': (106, 128), 'russia': (249, 190)},
    'Mexican': {'border': (90, 270), 'migrants': (75, 35), 'government': (0, 46)},
    'Venezuelan': {'migrants': (15, 67), 'government': (0, 33), 'gang': (0, 21)}
}

border_before = [90, 72, 90, 16]
border_after = [0, 0, 270, 20]

x = np.arange(len(groups))
width = 0.35

bars1 = ax2.bar(x - width/2, border_before, width, label='Before', alpha=0.8)
bars2 = ax2.bar(x + width/2, border_after, width, label='After', alpha=0.8)

ax2.set_xlabel('Group')
ax2.set_ylabel('Frequency')
ax2.set_title('Example: "Border" Before vs After', fontsize=12, fontweight='bold')
ax2.set_xticks(x)
ax2.set_xticklabels(groups, rotation=45, ha='right')
ax2.legend()

for bar in bars1:
    height = bar.get_height()
    if height > 0:
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}', ha='center', va='bottom', fontsize=8)
for bar in bars2:
    height = bar.get_height()
    if height > 0:
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}', ha='center', va='bottom', fontsize=8)

# Semantic profiles
ax3 = plt.subplot(2, 3, 4)

semantic_categories = {
    'Palestinian': {'violence': 613, 'geographic': 1013, 'political': 590, 'humanitarian': 197},
    'Ukrainian': {'violence': 476, 'political': 721, 'military': 397, 'geographic': 262},
    'Mexican': {'border': 360, 'political': 182, 'migration': 127, 'geographic': 60},
    'Venezuelan': {'migration': 82, 'political': 108, 'numeric': 34, 'border': 36}
}

categories = ['violence', 'political', 'border/migration', 'geographic', 'other']
data_matrix = []

for group in ['Palestinian', 'Ukrainian', 'Mexican', 'Venezuelan']:
    total = sum(semantic_categories[group].values())
    normalized = [semantic_categories[group].get(cat, 0)/total*100 for cat in categories[:4]]
    normalized.append(100 - sum(normalized))
    data_matrix.append(normalized)

data_matrix = np.array(data_matrix).T

bottom = np.zeros(4)
for i, cat in enumerate(categories):
    ax3.bar(range(4), data_matrix[i], bottom=bottom,
           label=cat.capitalize(), alpha=0.8)
    bottom += data_matrix[i]

ax3.set_xlabel('Group')
ax3.set_ylabel('Percentage of Collocations')
ax3.set_title('Semantic Domain Distribution', fontsize=12, fontweight='bold')
ax3.set_xticks(range(4))
ax3.set_xticklabels(['Palestinian', 'Ukrainian', 'Mexican', 'Venezuelan'], rotation=45, ha='right')
ax3.legend(loc='upper right', fontsize=8)

# agency indicators
ax4 = plt.subplot(2, 3, 5)

# active vs passive construction
agency_data = {
    'Palestinian': {'passive': 613, 'active': 197},  # killed, attacked vs humanitarian
    'Ukrainian': {'passive': 139, 'active': 553},     # invasion vs president, defense
    'Mexican': {'passive': 39, 'active': 157},        # authorities vs border, crossing
    'Venezuelan': {'passive': 57, 'active': 131}      # deported, held vs president, leader
}

x = np.arange(4)
passive_vals = [agency_data[g]['passive'] for g in ['Palestinian', 'Ukrainian', 'Mexican', 'Venezuelan']]
active_vals = [agency_data[g]['active'] for g in ['Palestinian', 'Ukrainian', 'Mexican', 'Venezuelan']]

bars1 = ax4.bar(x - width/2, passive_vals, width, label='Passive indicators', color='#E74C3C', alpha=0.8)
bars2 = ax4.bar(x + width/2, active_vals, width, label='Active indicators', color='#2ECC71', alpha=0.8)

ax4.set_xlabel('Group')
ax4.set_ylabel('Frequency')
ax4.set_title('Agency Indicators in Collocations', fontsize=12, fontweight='bold')
ax4.set_xticks(x)
ax4.set_xticklabels(['Palestinian', 'Ukrainian', 'Mexican', 'Venezuelan'], rotation=45, ha='right')
ax4.legend()

# distinctive terms
ax5 = plt.subplot(2, 3, 6)
ax5.axis('tight')
ax5.axis('off')

distinctive_data = [
    ['Palestinian', 'gaza, strip, hamas, humanitarian, israeli, killed, health, ministry'],
    ['Ukrainian', 'president, zelensky, defense, forces, military, official, kyiv'],
    ['Mexican', 'border, crossing, tariffs, sheinbaum, authorities, canada'],
    ['Venezuelan', '350000, temporary, gang, abroad, opposition, deported']
]

distinctive_table = ax5.table(cellText=distinctive_data,
                             colLabels=['Group', 'Distinctive Vocabulary'],
                             cellLoc='left',
                             loc='center',
                             colWidths=[0.2, 0.8])

distinctive_table.auto_set_font_size(False)
distinctive_table.set_fontsize(9)
distinctive_table.scale(1, 2)

for i in range(2):
    distinctive_table[(0, i)].set_facecolor('#40466e')
    distinctive_table[(0, i)].set_text_props(weight='bold', color='white')

ax5.set_title('Distinctive Terms by Group', fontsize=12, fontweight='bold', pad=20)

fig.suptitle('Collocation Analysis: Words Before and After Group Mentions',
            fontsize=16, fontweight='bold', y=0.98)

plt.tight_layout()

output_path = "/content/drive/MyDrive/Research/AMLI/figures/"
os.makedirs(output_path, exist_ok=True)
plt.savefig(f'{output_path}collocation_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n" + "="*70)
print("COLLOCATION PATTERNS SUMMARY")
print("="*70)

summary_df = pd.DataFrame({
    'Group': ['Palestinian', 'Ukrainian', 'Mexican', 'Venezuelan'],
    'Top Before': ['israeli(362), northern(272), killed(267)',
                   'russia(249), russian(173), support(144)',
                   'border(90), migrants(75), tariffs(35)',
                   'temporary(17), 350000(17), border(16)'],
    'Top After': ['israeli(417), gaza(400), strip(368)',
                  'president(222), russian(196), forces(163)',
                  'border(270), president(67), government(46)',
                  'migrants(67), government(33), president(31)'],
    'Key Pattern': ['Violence/Location focused',
                    'Political/Military authority',
                    'Border enforcement dominant',
                    'Migration/Political crisis']
})

print(summary_df.to_string(index=False))

print("\nâœ“ Visualizations saved to:", output_path)

DATA_PATH = "/content/drive/MyDrive/Research/AMLI/data/cnn_guardian_corpus.csv"
df = pd.read_csv(DATA_PATH)

print("="*70)
print("ADJECTIVE AND VERB COLLOCATION ANALYSIS")
print("="*70)

groups = {
    'palestinian': ['palestinian', 'palestinians', 'gaza', 'gazan', 'gazans'],
    'ukrainian': ['ukrainian', 'ukrainians', 'ukraine'],
    'mexican': ['mexican', 'mexicans', 'mexico'],
    'venezuelan': ['venezuelan', 'venezuelans', 'venezuela']
}

adjectives = {
    'positive_humanitarian': [
        'innocent', 'legitimate', 'genuine', 'peaceful', 'civilian', 'desperate',
        'vulnerable', 'humanitarian', 'legal', 'documented', 'authorized',
        'courageous', 'brave', 'heroic', 'worthy', 'deserving'
    ],
    'negative_criminalizing': [
        'illegal', 'unauthorized', 'undocumented', 'criminal', 'dangerous',
        'violent', 'hostile', 'radical', 'extremist', 'terrorist', 'threatening',
        'suspicious', 'fraudulent', 'bogus', 'fake', 'false'
    ],
    'descriptive_neutral': [
        'young', 'elderly', 'male', 'female', 'foreign', 'local', 'armed',
        'northern', 'southern', 'eastern', 'western', 'central', 'new',
        'former', 'current', 'recent', 'temporary', 'permanent'
    ]
}

verbs = {
    'active_movement': [
        'flee', 'fled', 'fleeing', 'escape', 'escaped', 'escaping',
        'cross', 'crossed', 'crossing', 'arrive', 'arrived', 'arriving',
        'leave', 'left', 'leaving', 'travel', 'traveled', 'traveling',
        'seek', 'seeking', 'sought', 'move', 'moved', 'moving'
    ],
    'passive_victim': [
        'killed', 'died', 'dying', 'wounded', 'injured', 'attacked',
        'bombed', 'targeted', 'struck', 'destroyed', 'damaged', 'hurt',
        'harmed', 'suffered', 'trapped', 'stranded', 'abandoned'
    ],
    'state_control': [
        'detained', 'detaining', 'arrested', 'arresting', 'deported', 'deporting',
        'expelled', 'expelling', 'processed', 'processing', 'apprehended',
        'intercepted', 'intercepting', 'blocked', 'blocking', 'stopped', 'stopping',
        'removed', 'removing', 'held', 'holding', 'caught', 'catching'
    ],
    'threat_action': [
        'flood', 'flooded', 'flooding', 'surge', 'surged', 'surging',
        'invade', 'invaded', 'invading', 'swarm', 'swarmed', 'swarming',
        'pour', 'poured', 'pouring', 'overwhelm', 'overwhelming', 'overwhelmed'
    ]
}

def analyze_adjectives(group_name, group_terms, df):
    """Extract adjectives near group mentions"""

    pattern = '|'.join([r'\b' + term + r'\b' for term in group_terms])
    group_docs = df[df['text'].str.contains(pattern, case=False, na=False, regex=True)]

    if len(group_docs) == 0:
        return None

    adj_counts = {cat: Counter() for cat in adjectives.keys()}
    total_counts = Counter()

    for text in group_docs['text'].dropna():
        text_lower = text.lower()


        for term in group_terms:

            for match in re.finditer(r'\b' + term + r'\b', text_lower):
                start = max(0, match.start() - 100)
                end = min(len(text_lower), match.end() + 100)
                context = text_lower[start:end]


                for cat, adj_list in adjectives.items():
                    for adj in adj_list:
                        if adj in context:
                            adj_counts[cat][adj] += context.count(adj)
                            total_counts[adj] += context.count(adj)

    return {
        'total': total_counts.most_common(15),
        'by_category': {cat: counter.most_common(10) for cat, counter in adj_counts.items()},
        'category_totals': {cat: sum(counter.values()) for cat, counter in adj_counts.items()}
    }

def analyze_verbs(group_name, group_terms, df):
    """Extract verbs near group mentions"""

    pattern = '|'.join([r'\b' + term + r'\b' for term in group_terms])
    group_docs = df[df['text'].str.contains(pattern, case=False, na=False, regex=True)]

    if len(group_docs) == 0:
        return None

    verb_counts = {cat: Counter() for cat in verbs.keys()}
    total_counts = Counter()
    before_counts = Counter()  # Verbs before group (object position)
    after_counts = Counter()   # Verbs after group (subject position)

    for text in group_docs['text'].dropna():
        text_lower = text.lower()
        words = text_lower.split()

        for term in group_terms:

            for i, word in enumerate(words):
                if term in word:
                    before_window = words[max(0, i-5):i]
                    for w in before_window:
                        for cat, verb_list in verbs.items():
                            for verb in verb_list:
                                if verb == w.strip('.,!?;:"()[]{}'):
                                    verb_counts[cat][verb] += 1
                                    total_counts[verb] += 1
                                    before_counts[verb] += 1


                    after_window = words[i+1:min(len(words), i+6)]
                    for w in after_window:
                        for cat, verb_list in verbs.items():
                            for verb in verb_list:
                                if verb == w.strip('.,!?;:"()[]{}'):
                                    verb_counts[cat][verb] += 1
                                    total_counts[verb] += 1
                                    after_counts[verb] += 1

    return {
        'total': total_counts.most_common(15),
        'by_category': {cat: counter.most_common(10) for cat, counter in verb_counts.items()},
        'category_totals': {cat: sum(counter.values()) for cat, counter in verb_counts.items()},
        'before_group': before_counts.most_common(10),
        'after_group': after_counts.most_common(10)
    }


print("\n1. ADJECTIVE ANALYSIS")
print("="*50)

adj_results = {}
for group_name, group_terms in groups.items():
    print(f"\n{group_name.upper()}:")
    result = analyze_adjectives(group_name, group_terms, df)

    if result:
        adj_results[group_name] = result

        print(f"  Top adjectives overall:")
        for adj, count in result['total'][:8]:
            print(f"    {adj}: {count}")

        print(f"\n  By category:")
        for cat, total in result['category_totals'].items():
            if total > 0:
                print(f"    {cat}: {total}")

print("\n2. VERB ANALYSIS")
print("="*50)

verb_results = {}
for group_name, group_terms in groups.items():
    print(f"\n{group_name.upper()}:")
    result = analyze_verbs(group_name, group_terms, df)

    if result:
        verb_results[group_name] = result

        print(f"  Top verbs overall:")
        for verb, count in result['total'][:8]:
            print(f"    {verb}: {count}")

        print(f"\n  By category:")
        for cat, total in result['category_totals'].items():
            if total > 0:
                print(f"    {cat}: {total}")
        before_total = sum(count for _, count in result['before_group'])
        after_total = sum(count for _, count in result['after_group'])
        if before_total + after_total > 0:
            agency_ratio = after_total / (before_total + after_total) * 100
            print(f"\n  Agency indicator (verbs after = subject): {agency_ratio:.1f}%")

print("\n3. CREATING VISUALIZATIONS...")
print("="*50)

fig, axes = plt.subplots(2, 3, figsize=(16, 10))
ax1 = axes[0, 0]
if adj_results:
    groups_list = list(adj_results.keys())
    categories = ['positive_humanitarian', 'negative_criminalizing', 'descriptive_neutral']

    data = []
    for cat in categories:
        data.append([adj_results[g]['category_totals'][cat] for g in groups_list])

    x = np.arange(len(groups_list))
    width = 0.25
    colors = ['#2ecc71', '#e74c3c', '#95a5a6']

    for i, (cat_data, cat, color) in enumerate(zip(data, categories, colors)):
        ax1.bar(x + i*width - width, cat_data, width,
                label=cat.replace('_', ' ').title(), color=color, alpha=0.8)

    ax1.set_xlabel('Group', fontsize=11)
    ax1.set_ylabel('Frequency', fontsize=11)
    ax1.set_title('Adjective Categories', fontsize=12, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels([g.capitalize() for g in groups_list])
    ax1.legend(fontsize=9)
    ax1.grid(axis='y', alpha=0.3)

#  Verb categories
ax2 = axes[0, 1]
if verb_results:
    groups_list = list(verb_results.keys())
    categories = ['active_movement', 'passive_victim', 'state_control', 'threat_action']

    data = []
    for cat in categories:
        data.append([verb_results[g]['category_totals'][cat] for g in groups_list])

    bottom = np.zeros(len(groups_list))
    colors = ['#3498db', '#e74c3c', '#f39c12', '#9b59b6']

    for cat_data, cat, color in zip(data, categories, colors):
        ax2.bar(groups_list, cat_data, bottom=bottom,
                label=cat.replace('_', ' ').title(), color=color, alpha=0.8)
        bottom += np.array(cat_data)

    ax2.set_xlabel('Group', fontsize=11)
    ax2.set_ylabel('Frequency', fontsize=11)
    ax2.set_title('Verb Category Distribution', fontsize=12, fontweight='bold')
    ax2.set_xticklabels([g.capitalize() for g in groups_list])
    ax2.legend(fontsize=9, loc='upper right')
    ax2.grid(axis='y', alpha=0.3)

# Agency indicator
ax3 = axes[0, 2]
if verb_results:
    groups_list = list(verb_results.keys())

    before_totals = []
    after_totals = []

    for g in groups_list:
        before_totals.append(sum(count for _, count in verb_results[g]['before_group']))
        after_totals.append(sum(count for _, count in verb_results[g]['after_group']))

    x = np.arange(len(groups_list))
    width = 0.35

    bars1 = ax3.bar(x - width/2, before_totals, width,
                    label='Before (Object)', color='#e74c3c', alpha=0.8)
    bars2 = ax3.bar(x + width/2, after_totals, width,
                    label='After (Subject)', color='#2ecc71', alpha=0.8)

    ax3.set_xlabel('Group', fontsize=11)
    ax3.set_ylabel('Frequency', fontsize=11)
    ax3.set_title('Grammatical Agency Pattern', fontsize=12, fontweight='bold')
    ax3.set_xticks(x)
    ax3.set_xticklabels([g.capitalize() for g in groups_list])
    ax3.legend(fontsize=9)
    ax3.grid(axis='y', alpha=0.3)

# top adj table
ax4 = axes[1, 0]
ax4.axis('tight')
ax4.axis('off')

if adj_results:
    table_data = []
    for g in groups_list:
        top_adj = ', '.join([adj for adj, _ in adj_results[g]['total'][:5]])
        table_data.append([g.capitalize(), top_adj])

    table = ax4.table(cellText=table_data,
                     colLabels=['Group', 'Top Adjectives'],
                     cellLoc='left',
                     loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1, 1.8)
    ax4.set_title('Most Frequent Adjectives', fontsize=11, fontweight='bold')

# top verbs table
ax5 = axes[1, 1]
ax5.axis('tight')
ax5.axis('off')

if verb_results:
    table_data = []
    for g in groups_list:
        top_verbs = ', '.join([verb for verb, _ in verb_results[g]['total'][:5]])
        table_data.append([g.capitalize(), top_verbs])

    table = ax5.table(cellText=table_data,
                     colLabels=['Group', 'Top Verbs'],
                     cellLoc='left',
                     loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1, 1.8)
    ax5.set_title('Most Frequent Verbs', fontsize=11, fontweight='bold')

# summary
ax6 = axes[1, 2]
ax6.axis('tight')
ax6.axis('off')

if adj_results and verb_results:
    summary_data = []
    for g in groups_list:
        pos = adj_results[g]['category_totals']['positive_humanitarian']
        neg = adj_results[g]['category_totals']['negative_criminalizing']
        passive = verb_results[g]['category_totals']['passive_victim']
        active = verb_results[g]['category_totals']['active_movement']

        eval_ratio = pos / (pos + neg) * 100 if (pos + neg) > 0 else 0
        agency_ratio = active / (active + passive) * 100 if (active + passive) > 0 else 0

        summary_data.append([g.capitalize(), f"{eval_ratio:.0f}%", f"{agency_ratio:.0f}%"])

    table = ax6.table(cellText=summary_data,
                     colLabels=['Group', 'Positive Adj %', 'Active Verb %'],
                     cellLoc='center',
                     loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 1.8)
    ax6.set_title('Key Metrics', fontsize=11, fontweight='bold')

plt.suptitle('Adjective and Verb Collocation Analysis', fontsize=14, fontweight='bold')
plt.tight_layout()

output_path = "/content/drive/MyDrive/Research/AMLI/figures/"
os.makedirs(output_path, exist_ok=True)
plt.savefig(f'{output_path}adjective_verb_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\nâœ“ Analysis complete. Visualization saved to: {output_path}")

print("\n" + "="*70)
print("KEY FINDINGS SUMMARY")
print("="*70)

for group in groups.keys():
    if group in adj_results and group in verb_results:
        print(f"\n{group.upper()}:")

        # Adjective summary
        pos = adj_results[group]['category_totals']['positive_humanitarian']
        neg = adj_results[group]['category_totals']['negative_criminalizing']
        print(f"  Adjectives: {pos} positive, {neg} negative")

        # Verb summary
        active = verb_results[group]['category_totals']['active_movement']
        passive = verb_results[group]['category_totals']['passive_victim']
        control = verb_results[group]['category_totals']['state_control']
        print(f"  Verbs: {active} active movement, {passive} passive victim, {control} state control")

"""---

# TRANSITIVITY ANALYSIS
"""

DATA_PATH = "/content/drive/MyDrive/Research/AMLI/data/cnn_guardian_corpus.csv"
df = pd.read_csv(DATA_PATH)

print("="*70)
print("TRANSITIVITY ANALYSIS - GRAMMATICAL AGENCY")
print("="*70)

groups = {
    'palestinian': ['palestinian', 'palestinians', 'gaza', 'gazan', 'gazans'],
    'ukrainian': ['ukrainian', 'ukrainians', 'ukraine'],
    'mexican': ['mexican', 'mexicans', 'mexico'],
    'venezuelan': ['venezuelan', 'venezuelans', 'venezuela']
}

transitivity_patterns = {
    'agent_active': {

        'patterns': [
            r'({group})\s+(fled|flee|fleeing)',
            r'({group})\s+(crossed|cross|crossing)',
            r'({group})\s+(arrived|arrive|arriving)',
            r'({group})\s+(sought|seek|seeking)',
            r'({group})\s+(escaped|escape|escaping)',
            r'({group})\s+(moved|move|moving)',
            r'({group})\s+(traveled|travel|traveling)',
            r'({group})\s+(left|leave|leaving)',
            r'({group})\s+(came|come|coming)',
            r'({group})\s+(entered|enter|entering)'
        ],
        'examples': []
    },
    'patient_passive': {

        'patterns': [
            r'(killed|killing)\s+.*?({group})',
            r'(detained|detaining)\s+.*?({group})',
            r'(deported|deporting)\s+.*?({group})',
            r'(arrested|arresting)\s+.*?({group})',
            r'(attacked|attacking)\s+.*?({group})',
            r'(blocked|blocking)\s+.*?({group})',
            r'(stopped|stopping)\s+.*?({group})',
            r'(held|holding)\s+.*?({group})',
            r'(targeted|targeting)\s+.*?({group})',
            r'({group})\s+(were|was)\s+(killed|detained|deported|arrested|attacked)'
        ],
        'examples': []
    },
    'agent_negative': {

        'patterns': [
            r'({group})\s+(flooded|flood|flooding)',
            r'({group})\s+(surged|surge|surging)',
            r'({group})\s+(poured|pour|pouring)',
            r'({group})\s+(swarmed|swarm|swarming)',
            r'({group})\s+(overwhelmed|overwhelm|overwhelming)',
            r'({group})\s+(invaded|invade|invading)'
        ],
        'examples': []
    }
}

def analyze_transitivity(group_name, group_terms, df):
    """Analyze transitivity patterns for a group"""

    pattern = '|'.join([r'\b' + term + r'\b' for term in group_terms])
    group_docs = df[df['text'].str.contains(pattern, case=False, na=False, regex=True)]

    if len(group_docs) == 0:
        return None

    transitivity_counts = {cat: 0 for cat in transitivity_patterns.keys()}
    examples = {cat: [] for cat in transitivity_patterns.keys()}

    for text in group_docs['text'].dropna()[:300]:
        sentences = re.split(r'[.!?]+', text)

        for sent in sentences[:100]:
            sent_lower = sent.lower()


            for category, info in transitivity_patterns.items():
                for pattern_template in info['patterns']:

                    for term in group_terms:
                        pattern = pattern_template.replace('{group}', term)

                        if re.search(pattern, sent_lower):
                            transitivity_counts[category] += 1

                            if len(examples[category]) < 3 and len(sent) < 200:
                                examples[category].append(sent.strip())
                            break

    return {
        'counts': transitivity_counts,
        'examples': examples,
        'n_articles': len(group_docs),
        'total': sum(transitivity_counts.values())
    }

results = {}
print("\n1. TRANSITIVITY PATTERNS BY GROUP")
print("-"*50)

for group_name, group_terms in groups.items():
    result = analyze_transitivity(group_name, group_terms, df)

    if result and result['total'] > 0:
        results[group_name] = result

        print(f"\n{group_name.upper()}:")
        print(f"  Articles analyzed: {result['n_articles']}")
        print(f"  Total patterns found: {result['total']}")


        for cat, count in result['counts'].items():
            pct = (count/result['total'])*100 if result['total'] > 0 else 0
            print(f"    {cat}: {count} ({pct:.1f}%)")


print("\n2. AGENCY SCORES")
print("="*70)

agency_scores = []
for group in ['palestinian', 'ukrainian', 'mexican', 'venezuelan']:
    if group in results:
        counts = results[group]['counts']
        total = results[group]['total']

        if total > 0:
            # Active agency = agent_active / total
            active = counts['agent_active'] / total * 100
            # Passive = patient_passive / total
            passive = counts['patient_passive'] / total * 100
            # Negative agency = agent_negative / total
            negative = counts['agent_negative'] / total * 100

            agency_scores.append({
                'Group': group.capitalize(),
                'Active Agency %': f"{active:.1f}",
                'Passive (Patient) %': f"{passive:.1f}",
                'Negative Agency %': f"{negative:.1f}",
                'Agency Ratio': f"{active/(passive+0.1):.2f}"
            })

agency_df = pd.DataFrame(agency_scores)
print(agency_df.to_string(index=False))
print("\n3. EXAMPLE SENTENCES")
print("="*70)

for group in ['palestinian', 'ukrainian', 'mexican', 'venezuelan']:
    if group in results:
        print(f"\n{group.upper()} EXAMPLES:")

        for cat in ['agent_active', 'patient_passive', 'agent_negative']:
            if results[group]['examples'][cat]:
                ex = results[group]['examples'][cat][0]
                if len(ex) > 150:
                    ex = ex[:150] + "..."
                print(f"  {cat}: \"{ex}\"")

print("\n4. CREATING VISUALIZATIONS...")
print("="*70)

fig, axes = plt.subplots(2, 2, figsize=(14, 10))


ax1 = axes[0, 0]
groups_list = list(results.keys())
categories = ['agent_active', 'patient_passive', 'agent_negative']
labels = ['Active Agent', 'Passive Patient', 'Negative Agent']
colors = ['#2ecc71', '#e74c3c', '#f39c12']

data = []
for cat in categories:
    data.append([results[g]['counts'][cat] for g in groups_list])

x = np.arange(len(groups_list))
bottom = np.zeros(len(groups_list))

for cat_data, label, color in zip(data, labels, colors):
    ax1.bar(x, cat_data, bottom=bottom, label=label, color=color, alpha=0.8)
    bottom += np.array(cat_data)

ax1.set_xlabel('Group', fontsize=11)
ax1.set_ylabel('Frequency', fontsize=11)
ax1.set_title('Transitivity Pattern Distribution', fontsize=12, fontweight='bold')
ax1.set_xticks(x)
ax1.set_xticklabels([g.capitalize() for g in groups_list])
ax1.legend()
ax1.grid(axis='y', alpha=0.3)

# Agency ratio comparison
ax2 = axes[0, 1]
if agency_scores:
    groups_display = [a['Group'] for a in agency_scores]
    active_vals = [float(a['Active Agency %']) for a in agency_scores]
    passive_vals = [float(a['Passive (Patient) %']) for a in agency_scores]

    x = np.arange(len(groups_display))
    width = 0.35

    bars1 = ax2.bar(x - width/2, active_vals, width, label='Active Agent',
                    color='#2ecc71', alpha=0.8)
    bars2 = ax2.bar(x + width/2, passive_vals, width, label='Passive Patient',
                    color='#e74c3c', alpha=0.8)

    ax2.set_xlabel('Group', fontsize=11)
    ax2.set_ylabel('Percentage (%)', fontsize=11)
    ax2.set_title('Active Agent vs Passive Patient', fontsize=12, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels(groups_display)
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)

    # Add value labels
    for bar in bars1:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.0f}%', ha='center', va='bottom', fontsize=9)
    for bar in bars2:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.0f}%', ha='center', va='bottom', fontsize=9)

#  Agency spectrum
ax3 = axes[1, 0]
if agency_scores:
    active_vals = [float(a['Active Agency %']) for a in agency_scores]
    passive_vals = [float(a['Passive (Patient) %']) for a in agency_scores]

    # Calculate net agency (active - passive)
    net_agency = [a - p for a, p in zip(active_vals, passive_vals)]

    colors_bar = ['#E74C3C', '#3498DB', '#2ECC71', '#F39C12']
    bars = ax3.barh(range(len(groups_display)), net_agency, color=colors_bar, alpha=0.8)

    ax3.set_yticks(range(len(groups_display)))
    ax3.set_yticklabels(groups_display)
    ax3.set_xlabel('Net Agency (Active % - Passive %)', fontsize=11)
    ax3.set_title('Agency Spectrum', fontsize=12, fontweight='bold')
    ax3.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
    ax3.grid(axis='x', alpha=0.3)

    for i, (bar, val) in enumerate(zip(bars, net_agency)):
        ax3.text(val + (2 if val > 0 else -2), bar.get_y() + bar.get_height()/2,
                f'{val:.0f}', ha='left' if val > 0 else 'right', va='center', fontsize=10)

# Summary table
ax4 = axes[1, 1]
ax4.axis('tight')
ax4.axis('off')

if agency_scores:
    summary_data = []
    for a in agency_scores:
        summary_data.append([
            a['Group'],
            a['Active Agency %'],
            a['Passive (Patient) %'],
            a['Agency Ratio']
        ])

    table = ax4.table(cellText=summary_data,
                     colLabels=['Group', 'Active %', 'Passive %', 'Ratio'],
                     cellLoc='center',
                     loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)

    for i in range(1, len(summary_data) + 1):
        ratio = float(summary_data[i-1][3])
        if ratio > 1:
            color = '#d5f4e6'
        elif ratio < 0.5:
            color = '#f8d7da'
        else:
            color = '#fff3cd'

        for j in range(4):
            table[(i, j)].set_facecolor(color)

    ax4.set_title('Transitivity Summary', fontsize=12, fontweight='bold')

plt.suptitle('Transitivity Analysis: Grammatical Agency Patterns', fontsize=14, fontweight='bold')
plt.tight_layout()

output_path = "/content/drive/MyDrive/Research/AMLI/figures/"
os.makedirs(output_path, exist_ok=True)
plt.savefig(f'{output_path}transitivity_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\nâœ“ Visualization saved to: {output_path}")


print("\n5. KEY FINDINGS")
print("="*70)

if agency_scores:

    max_active = max(agency_scores, key=lambda x: float(x['Active Agency %']))
    max_passive = max(agency_scores, key=lambda x: float(x['Passive (Patient) %']))

    print(f"Highest active agency: {max_active['Group']} ({max_active['Active Agency %']}%)")
    print(f"Highest passive patient: {max_passive['Group']} ({max_passive['Passive (Patient) %']}%)")
    print("\nThis shows who has grammatical power to act vs who is acted upon.")

print("\nâœ“ Transitivity analysis complete")

"""# TEMPORAL ANALYSIS"""

df['date'] = pd.to_datetime(df['date'], errors='coerce')


event_date = pd.Timestamp("2022-02-24")

df_ukr = df[df['category']=="ukraine"].copy()
df_ukr['period'] = df_ukr['date'].apply(lambda x: "Before invasion" if x < event_date else "After invasion")

terms = ["refugee","refugees","migrant","migrants","asylum","immigrant","immigrants"]

def term_counts(sub, terms):
    counts = {}
    total_tokens = sub['text'].str.split().str.len().sum()
    for t in terms:
        raw = sub['text_lower'].str.count(rf"\b{t}\b").sum()
        norm = raw / max(total_tokens,1) * 10000
        counts[t] = norm
    return counts

ukr_results = []
for (outlet, period), sub in df_ukr.groupby(['outlet','period']):
    c = term_counts(sub, terms)
    for t, val in c.items():
        ukr_results.append({"outlet": outlet, "period": period, "term": t, "norm_per_10k": val})

df_ukr_res = pd.DataFrame(ukr_results)


plt.figure(figsize=(10,6))
sns.barplot(data=df_ukr_res, x="term", y="norm_per_10k", hue="period")
plt.title("Ukraine coverage â€” Lexical framing before/after Feb 24, 2022")
plt.ylabel("Frequency per 10k words")
plt.show()