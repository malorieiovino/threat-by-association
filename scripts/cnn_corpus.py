# -*- coding: utf-8 -*-
"""CNN_corpus.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QDKHSH_WoUnjIIbOGEfBEIVwVy7_gMPb
"""

from google.colab import drive
drive.mount('/content/gdrive')

SAVE_PATH = "/content/drive/MyDrive/Research/AMLI/data/cnn_corpus_2021_2025.csv"

!pip install requests newspaper3k trafilatura pandas tqdm -q

import requests
import pandas as pd
from tqdm import tqdm
from newspaper import Article
import trafilatura
import time
from dateutil.relativedelta import relativedelta
from datetime import datetime
import os, json
import sys
import requests
from requests.exceptions import RequestException

"""## Search Terms"""

SEARCH_TERMS = {
    'ukraine': {
        'terms': ['(Ukraine OR Ukrainian)'],
        'max_articles': 200
    },
    'palestine': {
        'terms': ['(Palestine OR Palestinian OR Gaza OR Gazan OR Hamas)'],
        'max_articles': 200
    },
    'latinamerica': {
        'terms': ['(Mexico OR Mexican OR Venezuela OR Venezuelan OR "US border" OR caravan OR border)'],
        'max_articles': 200
    },
    'general': {
        'terms': [
            '(migrant OR migrants OR refugee OR refugees OR asylum OR immigrant OR immigrants OR displaced)'
        ],
        'max_articles': 300
    }
}

from datetime import datetime as dt
from dateutil.relativedelta import relativedelta

def month_chunks(start_date='2021-01-01', end_date='2025-09-01', newest_first=True):
    """Yield (startdt, enddt) in GDELT YYYYMMDDHHMMSS. Optionally newest→oldest."""
    start = dt.strptime(start_date, '%Y-%m-%d')
    stop  = dt.strptime(end_date,   '%Y-%m-%d')

    wins = []
    cur = start
    while cur < stop:
        nxt = min(cur + relativedelta(months=1), stop)
        wins.append( (cur.strftime('%Y%m%d%H%M%S'), nxt.strftime('%Y%m%d%H%M%S')) )
        cur = nxt
    return reversed(wins) if newest_first else wins

YEARS = [2021, 2022, 2023, 2024, 2025]

def year_window(y):
    start = f"{y}-01-01"
    end   = f"{y}-12-31"
    if y == 2025:
        end = "2025-09-01"
    return start, end

def split_quota(total, years=YEARS):
    base = total // len(years)
    rem  = total - base*len(years)
    q = {y: base for y in years}
    for y in years[:rem]:
        q[y] += 1
    return q

GDELT_DOC_URL = "https://api.gdeltproject.org/api/v2/doc/doc"
HEADERS = {"User-Agent": "amli-research/1.0 (contact: you@example.com)"}

def query_gdelt_cnn(term, startdt, enddt, maxrecords=200, retries=4, backoff=1.6):
    """
    Query GDELT ArtList for CNN within [startdt, enddt].
    Returns a list of article dicts, or [] on failure.
    """

    q = f'{term} domain:cnn.com sourcelang:english'
    params = {
        "query": q,
        "mode": "ArtList",
        "format": "JSON",
        "maxrecords": min(maxrecords, 250),  # GDELT cap
        "startdatetime": startdt,  # YYYYMMDDHHMMSS
        "enddatetime": enddt
    }

    for attempt in range(1, retries + 1):
        try:
            r = requests.get(GDELT_DOC_URL, params=params, headers=HEADERS, timeout=30)
            status = r.status_code
            if status != 200:

                time.sleep(backoff**attempt)
                continue
            try:
                data = r.json()
                return data.get("articles", [])
            except ValueError:
                preview = r.text[:300].replace("\n", " ")
                print(f"[GDELT JSON parse error] Attempt {attempt} "
                      f"| HTTP {status} | Preview: {preview}", file=sys.stderr)
                time.sleep(backoff**attempt)
        except RequestException as e:
            print(f"[GDELT request error] Attempt {attempt}: {e}", file=sys.stderr)
            time.sleep(backoff**attempt)
    return []

# scrape full text
def fetch_text(url):

    try:
        art = Article(url)
        art.download(); art.parse()
        txt = (art.text or "").strip()
        if len(txt) > 400:  # require some length
            return txt
    except Exception:
        pass
    # Fallback: trafilatura
    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            txt = (trafilatura.extract(downloaded) or "").strip()
            if len(txt) > 400:
                return txt
    except Exception:
        pass
    return None

def harvest_cnn_by_year(search_terms_dict, per_month_limit=200, polite_delay=0.8):
    rows = []
    seen_urls = set()

    for category, cfg in search_terms_dict.items():
        total_target = cfg.get('max_articles', 200)
        per_year_target = split_quota(total_target)
        print(f"\n== {category} targets by year:", per_year_target)

        for term in cfg['terms']:
            for y, need in per_year_target.items():
                if need == 0:
                    continue
                start, end = year_window(y)
                got = 0
                print(f"\n[{category} • {y}] Need {need} | Term: {term} | Window {start} → {end}")
                for startdt, enddt in month_chunks(start, end, newest_first=True):
                    if got >= need:
                        break
                    articles = query_gdelt_cnn(term, startdt, enddt, maxrecords=per_month_limit)
                    if not articles:
                        continue
                    for art in tqdm(articles, leave=False):
                        if got >= need:
                            break
                        url = art.get('url')
                        if not url or url in seen_urls:
                            continue
                        seen_urls.add(url)
                        txt = fetch_text(url)
                        if not txt:
                            time.sleep(polite_delay)
                            continue
                        rows.append({
                            "category": category,
                            "query": term,
                            "date":   art.get("seendate"),
                            "title":  art.get("title"),
                            "url":    url,
                            "source": art.get("sourceCommonName"),
                            "language": art.get("lang"),
                            "text":   txt
                        })
                        got += 1
                        time.sleep(polite_delay)
                    time.sleep(0.3)
                print(f"  Collected {got}/{need} for {category} in {y}.")
    return pd.DataFrame(rows)

df = harvest_cnn_by_year(SEARCH_TERMS, per_month_limit=250, polite_delay=0.8)
df = df.drop_duplicates(subset=['url']).reset_index(drop=True)
df['date'] = pd.to_datetime(df['date'], errors='coerce')
print("Year spread:\n", df.groupby(['category', df['date'].dt.year]).size().unstack(fill_value=0))

SAVE_PATH = "/content/drive/MyDrive/Research/AMLI/data/cnn_corpus_2021_2025_balanced.csv"
df.to_csv(SAVE_PATH, index=False)
print("Saved →", SAVE_PATH)

!ls "/content/drive/MyDrive/Research/AMLI/data"

SAVE_PATH = "/content/drive/MyDrive/Research/AMLI/data/cnn_corpus_v2.csv"
df.to_csv(SAVE_PATH, index=False)
print("Saved to:", SAVE_PATH)

df = harvest_cnn(SEARCH_TERMS, start='2021-01-01', end=None, per_month_limit=250, polite_delay=0.8)
print("Total scraped:", len(df))
df = df.drop_duplicates(subset=['url']).reset_index(drop=True)

df.to_csv(SAVE_PATH, index=False)
print("Saved -> cnn_corpus_2021_2025.csv")

df.to_csv("cnn_corpus_2021_2025.csv", index=False)

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!cp cnn_corpus_2021_2025.csv "/content/drive/MyDrive/Research/AMLI/data/cnn_corpus_2021_2025.csv"

import os
print(os.path.exists("/content/drive/MyDrive/Research/AMLI/data/cnn_corpus_2021_2025.csv"))